{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JosephThompson607/BBR/blob/main/sepsis_vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "phRvmO49rUUT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we prepare the data for training"
      ],
      "metadata": {
        "id": "0SiJSH5EMXTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Read this from the cloud\n",
        "patients = pd.read_csv(\"/content/unique_patient_dem.csv\")\n",
        "\n",
        "patients.drop(columns=['subject_id'], inplace=True)\n",
        "numeric_cols = patients.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = patients.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "# Reorder DataFrame\n",
        "patients = patients[numeric_cols + categorical_cols]\n",
        "#1 hot encoding\n",
        "df_encoded = pd.get_dummies(patients, columns=['race', 'gender'])\n",
        "\n",
        "#If cuda is available, device is cuda, otherwise cpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "scaler = StandardScaler()\n",
        "df_encoded['anchor_age'] = scaler.fit_transform(df_encoded[['anchor_age']])\n",
        "features = df_encoded.astype('float32').values\n",
        "# print(features.columns)\n",
        "# print(features.dtypes)\n",
        "# Get indices for slicing\n",
        "num_indices = list(range(len(numeric_cols)))\n",
        "n_numeric = len(numeric_cols)\n",
        "cat_indices = list(range(len(numeric_cols), len(features)))\n",
        "tensor = torch.tensor(features, dtype=torch.float32)\n",
        "\n",
        "X_train, X_test = train_test_split(tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = TensorDataset(X_train)  # or (X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "input_size = X_train[0].shape[0] #input size is the number of features going into the network\n",
        "print(input_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j5hmBSfrZhl",
        "outputId": "998e232b-7b36-486f-d610-be35106e77ec"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we define the model and related functions"
      ],
      "metadata": {
        "id": "HeWHLkFqMPmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngf = 64\n",
        "ndf = 64\n",
        "nc = 1\n",
        "\n",
        "def prior(K, alpha):\n",
        "    \"\"\"\n",
        "    Prior for the model.\n",
        "    :K: number of categories\n",
        "    :alpha: Hyper param of Dir\n",
        "    :return: mean and variance tensors\n",
        "    \"\"\"\n",
        "    # ラプラス近似で正規分布に近似\n",
        "    # Approximate to normal distribution using Laplace approximation\n",
        "    a = torch.Tensor(1, K).float().fill_(alpha)\n",
        "    mean = a.log().t() - a.log().mean(1)\n",
        "    var = ((1 - 2.0 / K) * a.reciprocal()).t() + (1.0 / K ** 2) * a.reciprocal().sum(1)\n",
        "    return mean.t(), var.t() # Parameters of prior distribution after approximation\n",
        "\n",
        "class Dir_VAE(nn.Module):\n",
        "    def __init__(self, input_size,n_numeric, latent_size=10, hidden_dim = 200):\n",
        "        self.num_numeric_cols = n_numeric\n",
        "        self.latent_size = latent_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.input_size = input_size\n",
        "        super(Dir_VAE, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "          nn.Linear(self.input_size, self.hidden_dim),\n",
        "          nn.ReLU(),\n",
        "          # nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          # nn.ReLU(),\n",
        "          # nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          # nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "          nn.Linear(self.latent_size, self.hidden_dim),\n",
        "          nn.ReLU(),\n",
        "          # nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          # nn.ReLU(),\n",
        "          # nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          # nn.ReLU(),\n",
        "          nn.Linear(self.hidden_dim, self.input_size),\n",
        "\n",
        "          # nn.Unflatten(dim=1, unflattened_size=(1, 28, 28)) # This was for image data\n",
        "        )\n",
        "        #self.fc1 = nn.Linear(self.hidden_dim, 512)\n",
        "        self.fc21 = nn.Linear(self.hidden_dim, self.latent_size)\n",
        "        self.fc22 = nn.Linear(self.hidden_dim, self.latent_size)\n",
        "\n",
        "        #self.fc3 = nn.Linear(10, 512)\n",
        "        #self.fc4 = nn.Linear(512, self.hidden_dim)\n",
        "\n",
        "        self.lrelu = nn.LeakyReLU()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Dir prior\n",
        "        self.prior_mean, self.prior_var = map(nn.Parameter, prior(self.latent_size, 0.3)) # 0.3 is a hyper param of Dirichlet distribution\n",
        "        self.prior_logvar = nn.Parameter(self.prior_var.log())\n",
        "        self.prior_mean.requires_grad = False\n",
        "        self.prior_var.requires_grad = False\n",
        "        self.prior_logvar.requires_grad = False\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        encoding = self.encoder(x);\n",
        "        #h1 = self.fc1(encoding)\n",
        "        return self.fc21(encoding), self.fc22(encoding)\n",
        "\n",
        "    def decode(self, gauss_z):\n",
        "        dir_z = F.softmax(gauss_z,dim=1) #Reduntant, already done in forward\n",
        "        # This variable (z) can be treated as a variable that follows a Dirichlet distribution (a variable that can be interpreted as a probability that the sum is 1)\n",
        "        # Use the Softmax function to satisfy the simplex constraint\n",
        "        x_out = self.decoder(dir_z)\n",
        "        # Apply sigmoid to categorical output only\n",
        "        x_out[:, self.num_numeric_cols:] = torch.sigmoid(x_out[:, self.num_numeric_cols:])\n",
        "        return x_out\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        gauss_z = self.reparameterize(mu, logvar)\n",
        "        # gause_z is a variable that follows a multivariate normal distribution\n",
        "        # Inputting gause_z into softmax func yields a random variable that follows a Dirichlet distribution (Softmax func are used in decoder)\n",
        "        dir_z = F.softmax(gauss_z,dim=1) # This variable follows a Dirichlet distribution\n",
        "        return self.decode(gauss_z), mu, logvar, gauss_z, dir_z\n",
        "\n",
        "    def reconstruction_loss(self, x_true, x_recon):\n",
        "        # Slice the tensors\n",
        "        x_true_num = x_true[:, :self.num_numeric_cols]\n",
        "        x_true_cat = x_true[:, self.num_numeric_cols:]\n",
        "\n",
        "        x_recon_num = x_recon[:, :self.num_numeric_cols]\n",
        "        x_recon_cat = x_recon[:, self.num_numeric_cols:]\n",
        "\n",
        "        # Compute losses\n",
        "        num_loss = F.mse_loss(x_recon_num, x_true_num)\n",
        "        cat_loss = F.cross_entropy(x_recon_cat, x_true_cat)\n",
        "\n",
        "        return num_loss + cat_loss\n",
        "\n",
        "    # Reconstruction + KL divergence losses summed over all elements and batch\n",
        "    def loss_function(self, recon_x, x, mu, logvar):\n",
        "        # Apply sigmoid to the input data x to ensure values are between 0 and 1\n",
        "        recon_loss = self.reconstruction_loss(x, recon_x, )\n",
        "        # ディリクレ事前分布と変分事後分布とのKLを計算\n",
        "        # Calculating KL with Dirichlet prior and variational posterior distributions\n",
        "        # Original paper:\"Autoencodeing variational inference for topic model\"-https://arxiv.org/pdf/1703.01488\n",
        "        prior_mean = self.prior_mean.expand_as(mu)\n",
        "        prior_var = self.prior_var.expand_as(logvar)\n",
        "        prior_logvar = self.prior_logvar.expand_as(logvar)\n",
        "        var_division = logvar.exp() / prior_var # Σ_0 / Σ_1\n",
        "        diff = mu - prior_mean # μ_１ - μ_0\n",
        "        diff_term = diff *diff / prior_var # (μ_1 - μ_0)(μ_1 - μ_0)/Σ_1\n",
        "        logvar_division = prior_logvar - logvar # log|Σ_1| - log|Σ_0| = log(|Σ_1|/|Σ_2|)\n",
        "        # KL\n",
        "        KLD = 0.5 * ((var_division + diff_term + logvar_division).sum(1) - self.latent_size)\n",
        "        self.last_KLD = torch.mean(KLD) #Used for reporting\n",
        "        self.last_BCE = recon_loss\n",
        "        return recon_loss + KLD"
      ],
      "metadata": {
        "id": "jFoNCRlVMNud"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the training and test loop\n"
      ],
      "metadata": {
        "id": "EQ-82vOxMqkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = Dir_VAE(input_size, n_numeric, latent_size=3, hidden_dim=20).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data,) in enumerate(train_loader): # Unpack only one element\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar, gauss_z, dir_z = model(data)\n",
        "\n",
        "        loss = model.loss_function(recon_batch, data, mu, logvar, )\n",
        "        loss = loss.mean()\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            #print(f\"gause_z:{gauss_z[0]}\")\n",
        "            #print(f\"dir_z:{dir_z[0]},SUM:{torch.sum(dir_z[0])}\")\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader)}%)] \\\n",
        "            Loss:{loss.item() / len(data)}\\\n",
        "            R_loss {model.last_BCE}, KLD_loss {model.last_KLD}')\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (data,) in enumerate(test_loader): # Unpack only one element\n",
        "            data = data.to(device)\n",
        "            recon_batch, mu, logvar, gauss_z, dir_z = model(data)\n",
        "            loss = model.loss_function(recon_batch, data, mu, logvar)\n",
        "            test_loss += loss.mean()\n",
        "            test_loss.item()\n",
        "            if i == 0:\n",
        "                n = min(data.size(0), 18)\n",
        "                #comparison = torch.cat([data[:n],\n",
        "                #                      recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
        "                #save_image(comparison.cpu(),\n",
        "                #         'image/recon_' + str(epoch) + '.png', nrow=n)\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 学習(Train)\n",
        "    for epoch in range(1, 10 + 1):\n",
        "        train(epoch)\n",
        "        test(epoch)\n",
        "        #with torch.no_grad():\n",
        "            #sample = torch.randn(64, args.category).to(device)\n",
        "            #sample = model.decode(sample).cpu()\n",
        "            #save_image(sample.view(64, 1, 28, 28),'image/sample_' + str(epoch) + '.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgOw9CXb2p_l",
        "outputId": "a48399c7-d858-467c-e5ea-09d2ae3f65cf"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/14166 (0.0%)]             Loss:0.2660248875617981            R_loss 7.990550518035889, KLD_loss 0.5222454071044922\n",
            "Train Epoch: 1 [320/14166 (2.2573363431151243%)]             Loss:0.27752721309661865            R_loss 8.40882682800293, KLD_loss 0.472043514251709\n",
            "Train Epoch: 1 [640/14166 (4.514672686230249%)]             Loss:0.27103230357170105            R_loss 8.230645179748535, KLD_loss 0.44238895177841187\n",
            "Train Epoch: 1 [960/14166 (6.772009029345372%)]             Loss:0.2708148956298828            R_loss 8.259286880493164, KLD_loss 0.40678930282592773\n",
            "Train Epoch: 1 [1280/14166 (9.029345372460497%)]             Loss:0.2752244472503662            R_loss 8.44322681427002, KLD_loss 0.36395588517189026\n",
            "Train Epoch: 1 [1600/14166 (11.286681715575622%)]             Loss:0.2576388716697693            R_loss 7.924421310424805, KLD_loss 0.3200225234031677\n",
            "Train Epoch: 1 [1920/14166 (13.544018058690744%)]             Loss:0.25754955410957336            R_loss 7.958237171173096, KLD_loss 0.2833484709262848\n",
            "Train Epoch: 1 [2240/14166 (15.801354401805868%)]             Loss:0.25740376114845276            R_loss 8.002338409423828, KLD_loss 0.23458188772201538\n",
            "Train Epoch: 1 [2560/14166 (18.058690744920995%)]             Loss:0.24488666653633118            R_loss 7.6563496589660645, KLD_loss 0.18002355098724365\n",
            "Train Epoch: 1 [2880/14166 (20.31602708803612%)]             Loss:0.2517482042312622            R_loss 7.9200439453125, KLD_loss 0.13589875400066376\n",
            "Train Epoch: 1 [3200/14166 (22.573363431151243%)]             Loss:0.2418431043624878            R_loss 7.647461891174316, KLD_loss 0.09151750802993774\n",
            "Train Epoch: 1 [3520/14166 (24.830699774266364%)]             Loss:0.23642680048942566            R_loss 7.495131969451904, KLD_loss 0.07052642107009888\n",
            "Train Epoch: 1 [3840/14166 (27.08803611738149%)]             Loss:0.24416863918304443            R_loss 7.760166645050049, KLD_loss 0.053229864686727524\n",
            "Train Epoch: 1 [4160/14166 (29.345372460496613%)]             Loss:0.23532438278198242            R_loss 7.48798131942749, KLD_loss 0.042398445308208466\n",
            "Train Epoch: 1 [4480/14166 (31.602708803611737%)]             Loss:0.23078636825084686            R_loss 7.355691432952881, KLD_loss 0.029472168534994125\n",
            "Train Epoch: 1 [4800/14166 (33.86004514672686%)]             Loss:0.22887267172336578            R_loss 7.295498847961426, KLD_loss 0.02842680737376213\n",
            "Train Epoch: 1 [5120/14166 (36.11738148984199%)]             Loss:0.23980660736560822            R_loss 7.645503997802734, KLD_loss 0.028307069092988968\n",
            "Train Epoch: 1 [5440/14166 (38.37471783295711%)]             Loss:0.2287560999393463            R_loss 7.294881820678711, KLD_loss 0.02531382441520691\n",
            "Train Epoch: 1 [5760/14166 (40.63205417607224%)]             Loss:0.2293034940958023            R_loss 7.313027381896973, KLD_loss 0.024684350937604904\n",
            "Train Epoch: 1 [6080/14166 (42.88939051918736%)]             Loss:0.2180456966161728            R_loss 6.958691120147705, KLD_loss 0.018771115690469742\n",
            "Train Epoch: 1 [6400/14166 (45.146726862302486%)]             Loss:0.2230917513370514            R_loss 7.113900184631348, KLD_loss 0.025035839527845383\n",
            "Train Epoch: 1 [6720/14166 (47.40406320541761%)]             Loss:0.21960222721099854            R_loss 7.0013508796691895, KLD_loss 0.025919906795024872\n",
            "Train Epoch: 1 [7040/14166 (49.66139954853273%)]             Loss:0.22257980704307556            R_loss 7.09816312789917, KLD_loss 0.024390719830989838\n",
            "Train Epoch: 1 [7360/14166 (51.918735891647856%)]             Loss:0.2320326417684555            R_loss 7.38705587387085, KLD_loss 0.037988800555467606\n",
            "Train Epoch: 1 [7680/14166 (54.17607223476298%)]             Loss:0.2044890820980072            R_loss 6.526244163513184, KLD_loss 0.017406735569238663\n",
            "Train Epoch: 1 [8000/14166 (56.433408577878104%)]             Loss:0.20834025740623474            R_loss 6.646944046020508, KLD_loss 0.019943691790103912\n",
            "Train Epoch: 1 [8320/14166 (58.690744920993225%)]             Loss:0.2207680195569992            R_loss 7.030081748962402, KLD_loss 0.03449433296918869\n",
            "Train Epoch: 1 [8640/14166 (60.94808126410835%)]             Loss:0.21222293376922607            R_loss 6.763009071350098, KLD_loss 0.02812499925494194\n",
            "Train Epoch: 1 [8960/14166 (63.205417607223474%)]             Loss:0.22822590172290802            R_loss 7.258337020874023, KLD_loss 0.0448915958404541\n",
            "Train Epoch: 1 [9280/14166 (65.4627539503386%)]             Loss:0.21479538083076477            R_loss 6.838370323181152, KLD_loss 0.035081975162029266\n",
            "Train Epoch: 1 [9600/14166 (67.72009029345372%)]             Loss:0.22564491629600525            R_loss 7.171001434326172, KLD_loss 0.04963648319244385\n",
            "Train Epoch: 1 [9920/14166 (69.97742663656885%)]             Loss:0.2216305136680603            R_loss 7.040839195251465, KLD_loss 0.05133688822388649\n",
            "Train Epoch: 1 [10240/14166 (72.23476297968398%)]             Loss:0.21328864991664886            R_loss 6.782390117645264, KLD_loss 0.042846933007240295\n",
            "Train Epoch: 1 [10560/14166 (74.49209932279909%)]             Loss:0.21946007013320923            R_loss 6.961188316345215, KLD_loss 0.06153364107012749\n",
            "Train Epoch: 1 [10880/14166 (76.74943566591422%)]             Loss:0.21208545565605164            R_loss 6.734010696411133, KLD_loss 0.05272357538342476\n",
            "Train Epoch: 1 [11200/14166 (79.00677200902935%)]             Loss:0.21312236785888672            R_loss 6.756693363189697, KLD_loss 0.06322233378887177\n",
            "Train Epoch: 1 [11520/14166 (81.26410835214448%)]             Loss:0.2334972769021988            R_loss 7.382103443145752, KLD_loss 0.08980920165777206\n",
            "Train Epoch: 1 [11840/14166 (83.52144469525959%)]             Loss:0.21651983261108398            R_loss 6.852596759796143, KLD_loss 0.07603789865970612\n",
            "Train Epoch: 1 [12160/14166 (85.77878103837472%)]             Loss:0.21096861362457275            R_loss 6.6854681968688965, KLD_loss 0.06552782654762268\n",
            "Train Epoch: 1 [12480/14166 (88.03611738148985%)]             Loss:0.20830576121807098            R_loss 6.5858306884765625, KLD_loss 0.07995334267616272\n",
            "Train Epoch: 1 [12800/14166 (90.29345372460497%)]             Loss:0.21448546648025513            R_loss 6.794625759124756, KLD_loss 0.06890957057476044\n",
            "Train Epoch: 1 [13120/14166 (92.55079006772009%)]             Loss:0.2151113599538803            R_loss 6.810049057006836, KLD_loss 0.07351447641849518\n",
            "Train Epoch: 1 [13440/14166 (94.80812641083521%)]             Loss:0.20149071514606476            R_loss 6.390879154205322, KLD_loss 0.0568234808743\n",
            "Train Epoch: 1 [13760/14166 (97.06546275395034%)]             Loss:0.21692070364952087            R_loss 6.848662376403809, KLD_loss 0.09279994666576385\n",
            "Train Epoch: 1 [14080/14166 (99.32279909706546%)]             Loss:0.23070858418941498            R_loss 7.249835014343262, KLD_loss 0.13283932209014893\n",
            "====> Epoch: 1 Average loss: 0.2303\n",
            "====> Test set loss: 0.2116\n",
            "Train Epoch: 2 [0/14166 (0.0%)]             Loss:0.2062254101037979            R_loss 6.510374546051025, KLD_loss 0.0888386070728302\n",
            "Train Epoch: 2 [320/14166 (2.2573363431151243%)]             Loss:0.2217930555343628            R_loss 6.9648284912109375, KLD_loss 0.13254937529563904\n",
            "Train Epoch: 2 [640/14166 (4.514672686230249%)]             Loss:0.21522429585456848            R_loss 6.768233776092529, KLD_loss 0.11894340813159943\n",
            "Train Epoch: 2 [960/14166 (6.772009029345372%)]             Loss:0.2108314037322998            R_loss 6.635837554931641, KLD_loss 0.11076724529266357\n",
            "Train Epoch: 2 [1280/14166 (9.029345372460497%)]             Loss:0.20821209251880646            R_loss 6.564861297607422, KLD_loss 0.0979253351688385\n",
            "Train Epoch: 2 [1600/14166 (11.286681715575622%)]             Loss:0.21094480156898499            R_loss 6.6614508628845215, KLD_loss 0.08878280967473984\n",
            "Train Epoch: 2 [1920/14166 (13.544018058690744%)]             Loss:0.2080160677433014            R_loss 6.560812950134277, KLD_loss 0.09570109099149704\n",
            "Train Epoch: 2 [2240/14166 (15.801354401805868%)]             Loss:0.21096698939800262            R_loss 6.64777946472168, KLD_loss 0.10316455364227295\n",
            "Train Epoch: 2 [2560/14166 (18.058690744920995%)]             Loss:0.21666356921195984            R_loss 6.769692897796631, KLD_loss 0.16354070603847504\n",
            "Train Epoch: 2 [2880/14166 (20.31602708803612%)]             Loss:0.21535266935825348            R_loss 6.759227275848389, KLD_loss 0.13205870985984802\n",
            "Train Epoch: 2 [3200/14166 (22.573363431151243%)]             Loss:0.2056734412908554            R_loss 6.491238594055176, KLD_loss 0.09031181037425995\n",
            "Train Epoch: 2 [3520/14166 (24.830699774266364%)]             Loss:0.20923727750778198            R_loss 6.550295829772949, KLD_loss 0.1452973335981369\n",
            "Train Epoch: 2 [3840/14166 (27.08803611738149%)]             Loss:0.21062591671943665            R_loss 6.628368854522705, KLD_loss 0.11166094988584518\n",
            "Train Epoch: 2 [4160/14166 (29.345372460496613%)]             Loss:0.19906410574913025            R_loss 6.261539459228516, KLD_loss 0.10851197689771652\n",
            "Train Epoch: 2 [4480/14166 (31.602708803611737%)]             Loss:0.20174750685691833            R_loss 6.32489013671875, KLD_loss 0.13103041052818298\n",
            "Train Epoch: 2 [4800/14166 (33.86004514672686%)]             Loss:0.20514678955078125            R_loss 6.439030170440674, KLD_loss 0.12566646933555603\n",
            "Train Epoch: 2 [5120/14166 (36.11738148984199%)]             Loss:0.21163737773895264            R_loss 6.615340232849121, KLD_loss 0.1570553183555603\n",
            "Train Epoch: 2 [5440/14166 (38.37471783295711%)]             Loss:0.20959100127220154            R_loss 6.4990949630737305, KLD_loss 0.20781713724136353\n",
            "Train Epoch: 2 [5760/14166 (40.63205417607224%)]             Loss:0.22120705246925354            R_loss 6.887758255004883, KLD_loss 0.19086715579032898\n",
            "Train Epoch: 2 [6080/14166 (42.88939051918736%)]             Loss:0.20755842328071594            R_loss 6.482757091522217, KLD_loss 0.15911222994327545\n",
            "Train Epoch: 2 [6400/14166 (45.146726862302486%)]             Loss:0.20443758368492126            R_loss 6.367127418518066, KLD_loss 0.17487511038780212\n",
            "Train Epoch: 2 [6720/14166 (47.40406320541761%)]             Loss:0.21342293918132782            R_loss 6.641443729400635, KLD_loss 0.1880902498960495\n",
            "Train Epoch: 2 [7040/14166 (49.66139954853273%)]             Loss:0.21508628129959106            R_loss 6.676149845123291, KLD_loss 0.20661088824272156\n",
            "Train Epoch: 2 [7360/14166 (51.918735891647856%)]             Loss:0.21023981273174286            R_loss 6.5430684089660645, KLD_loss 0.1846056580543518\n",
            "Train Epoch: 2 [7680/14166 (54.17607223476298%)]             Loss:0.21079374849796295            R_loss 6.535268783569336, KLD_loss 0.21013092994689941\n",
            "Train Epoch: 2 [8000/14166 (56.433408577878104%)]             Loss:0.20655909180641174            R_loss 6.348737716674805, KLD_loss 0.2611531615257263\n",
            "Train Epoch: 2 [8320/14166 (58.690744920993225%)]             Loss:0.2097344994544983            R_loss 6.5314860343933105, KLD_loss 0.18001815676689148\n",
            "Train Epoch: 2 [8640/14166 (60.94808126410835%)]             Loss:0.20336854457855225            R_loss 6.291181564331055, KLD_loss 0.21661138534545898\n",
            "Train Epoch: 2 [8960/14166 (63.205417607223474%)]             Loss:0.2132270634174347            R_loss 6.6356520652771, KLD_loss 0.1876140534877777\n",
            "Train Epoch: 2 [9280/14166 (65.4627539503386%)]             Loss:0.20987237989902496            R_loss 6.494815349578857, KLD_loss 0.2211012989282608\n",
            "Train Epoch: 2 [9600/14166 (67.72009029345372%)]             Loss:0.21797984838485718            R_loss 6.742297172546387, KLD_loss 0.23305824398994446\n",
            "Train Epoch: 2 [9920/14166 (69.97742663656885%)]             Loss:0.2040862739086151            R_loss 6.341021537780762, KLD_loss 0.18973945081233978\n",
            "Train Epoch: 2 [10240/14166 (72.23476297968398%)]             Loss:0.22974397242069244            R_loss 7.057850360870361, KLD_loss 0.2939568758010864\n",
            "Train Epoch: 2 [10560/14166 (74.49209932279909%)]             Loss:0.20818516612052917            R_loss 6.478096961975098, KLD_loss 0.18382886052131653\n",
            "Train Epoch: 2 [10880/14166 (76.74943566591422%)]             Loss:0.2080310732126236            R_loss 6.437557697296143, KLD_loss 0.21943703293800354\n",
            "Train Epoch: 2 [11200/14166 (79.00677200902935%)]             Loss:0.21065638959407806            R_loss 6.469289779663086, KLD_loss 0.2717144191265106\n",
            "Train Epoch: 2 [11520/14166 (81.26410835214448%)]             Loss:0.20785152912139893            R_loss 6.427493572235107, KLD_loss 0.2237551212310791\n",
            "Train Epoch: 2 [11840/14166 (83.52144469525959%)]             Loss:0.20695267617702484            R_loss 6.465393543243408, KLD_loss 0.1570921242237091\n",
            "Train Epoch: 2 [12160/14166 (85.77878103837472%)]             Loss:0.21054284274578094            R_loss 6.535414695739746, KLD_loss 0.20195640623569489\n",
            "Train Epoch: 2 [12480/14166 (88.03611738148985%)]             Loss:0.20240485668182373            R_loss 6.30308723449707, KLD_loss 0.1738678365945816\n",
            "Train Epoch: 2 [12800/14166 (90.29345372460497%)]             Loss:0.19727490842342377            R_loss 6.188423156738281, KLD_loss 0.12437386810779572\n",
            "Train Epoch: 2 [13120/14166 (92.55079006772009%)]             Loss:0.19999286532402039            R_loss 6.207878589630127, KLD_loss 0.19189320504665375\n",
            "Train Epoch: 2 [13440/14166 (94.80812641083521%)]             Loss:0.2123466283082962            R_loss 6.5437517166137695, KLD_loss 0.25134050846099854\n",
            "Train Epoch: 2 [13760/14166 (97.06546275395034%)]             Loss:0.20837585628032684            R_loss 6.466169834136963, KLD_loss 0.20185762643814087\n",
            "Train Epoch: 2 [14080/14166 (99.32279909706546%)]             Loss:0.20769618451595306            R_loss 6.409666061401367, KLD_loss 0.23661167919635773\n",
            "====> Epoch: 2 Average loss: 0.2098\n",
            "====> Test set loss: 0.2086\n",
            "Train Epoch: 3 [0/14166 (0.0%)]             Loss:0.1977938711643219            R_loss 6.144181728363037, KLD_loss 0.18522252142429352\n",
            "Train Epoch: 3 [320/14166 (2.2573363431151243%)]             Loss:0.20520834624767303            R_loss 6.342822074890137, KLD_loss 0.22384504973888397\n",
            "Train Epoch: 3 [640/14166 (4.514672686230249%)]             Loss:0.21112145483493805            R_loss 6.539507865905762, KLD_loss 0.216378316283226\n",
            "Train Epoch: 3 [960/14166 (6.772009029345372%)]             Loss:0.20671096444129944            R_loss 6.366002082824707, KLD_loss 0.24874860048294067\n",
            "Train Epoch: 3 [1280/14166 (9.029345372460497%)]             Loss:0.2083694338798523            R_loss 6.464605331420898, KLD_loss 0.20321643352508545\n",
            "Train Epoch: 3 [1600/14166 (11.286681715575622%)]             Loss:0.20842716097831726            R_loss 6.411067962646484, KLD_loss 0.25860074162483215\n",
            "Train Epoch: 3 [1920/14166 (13.544018058690744%)]             Loss:0.19973264634609222            R_loss 6.1750874519348145, KLD_loss 0.21635767817497253\n",
            "Train Epoch: 3 [2240/14166 (15.801354401805868%)]             Loss:0.2045455425977707            R_loss 6.309957981109619, KLD_loss 0.23549915850162506\n",
            "Train Epoch: 3 [2560/14166 (18.058690744920995%)]             Loss:0.2095998376607895            R_loss 6.459303379058838, KLD_loss 0.24789085984230042\n",
            "Train Epoch: 3 [2880/14166 (20.31602708803612%)]             Loss:0.2230936586856842            R_loss 6.777926445007324, KLD_loss 0.36107051372528076\n",
            "Train Epoch: 3 [3200/14166 (22.573363431151243%)]             Loss:0.21363916993141174            R_loss 6.520386219024658, KLD_loss 0.316067099571228\n",
            "Train Epoch: 3 [3520/14166 (24.830699774266364%)]             Loss:0.2120247483253479            R_loss 6.472283840179443, KLD_loss 0.3125080168247223\n",
            "Train Epoch: 3 [3840/14166 (27.08803611738149%)]             Loss:0.20960518717765808            R_loss 6.475217342376709, KLD_loss 0.23214861750602722\n",
            "Train Epoch: 3 [4160/14166 (29.345372460496613%)]             Loss:0.204894557595253            R_loss 6.385647296905518, KLD_loss 0.17097820341587067\n",
            "Train Epoch: 3 [4480/14166 (31.602708803611737%)]             Loss:0.21273073554039001            R_loss 6.548235893249512, KLD_loss 0.2591473460197449\n",
            "Train Epoch: 3 [4800/14166 (33.86004514672686%)]             Loss:0.207803413271904            R_loss 6.420764923095703, KLD_loss 0.22894452512264252\n",
            "Train Epoch: 3 [5120/14166 (36.11738148984199%)]             Loss:0.2065049409866333            R_loss 6.370100021362305, KLD_loss 0.23805800080299377\n",
            "Train Epoch: 3 [5440/14166 (38.37471783295711%)]             Loss:0.20373551547527313            R_loss 6.231882095336914, KLD_loss 0.2876543402671814\n",
            "Train Epoch: 3 [5760/14166 (40.63205417607224%)]             Loss:0.2152312844991684            R_loss 6.609938144683838, KLD_loss 0.277463436126709\n",
            "Train Epoch: 3 [6080/14166 (42.88939051918736%)]             Loss:0.2039591372013092            R_loss 6.3025360107421875, KLD_loss 0.22415605187416077\n",
            "Train Epoch: 3 [6400/14166 (45.146726862302486%)]             Loss:0.2015291154384613            R_loss 6.195584774017334, KLD_loss 0.2533469498157501\n",
            "Train Epoch: 3 [6720/14166 (47.40406320541761%)]             Loss:0.19755390286445618            R_loss 6.129830837249756, KLD_loss 0.19189414381980896\n",
            "Train Epoch: 3 [7040/14166 (49.66139954853273%)]             Loss:0.20223408937454224            R_loss 6.229156494140625, KLD_loss 0.24233385920524597\n",
            "Train Epoch: 3 [7360/14166 (51.918735891647856%)]             Loss:0.2018151879310608            R_loss 6.277824878692627, KLD_loss 0.18026156723499298\n",
            "Train Epoch: 3 [7680/14166 (54.17607223476298%)]             Loss:0.2070082426071167            R_loss 6.4387431144714355, KLD_loss 0.1855209320783615\n",
            "Train Epoch: 3 [8000/14166 (56.433408577878104%)]             Loss:0.20713888108730316            R_loss 6.351798057556152, KLD_loss 0.2766464650630951\n",
            "Train Epoch: 3 [8320/14166 (58.690744920993225%)]             Loss:0.20880720019340515            R_loss 6.492074966430664, KLD_loss 0.18975520133972168\n",
            "Train Epoch: 3 [8640/14166 (60.94808126410835%)]             Loss:0.20307478308677673            R_loss 6.3367919921875, KLD_loss 0.16160157322883606\n",
            "Train Epoch: 3 [8960/14166 (63.205417607223474%)]             Loss:0.19943124055862427            R_loss 6.177581310272217, KLD_loss 0.20421816408634186\n",
            "Train Epoch: 3 [9280/14166 (65.4627539503386%)]             Loss:0.2167544960975647            R_loss 6.681944370269775, KLD_loss 0.2541993260383606\n",
            "Train Epoch: 3 [9600/14166 (67.72009029345372%)]             Loss:0.2028864473104477            R_loss 6.289803981781006, KLD_loss 0.20256267488002777\n",
            "Train Epoch: 3 [9920/14166 (69.97742663656885%)]             Loss:0.2044040709733963            R_loss 6.294114589691162, KLD_loss 0.246815487742424\n",
            "Train Epoch: 3 [10240/14166 (72.23476297968398%)]             Loss:0.20928098261356354            R_loss 6.501862049102783, KLD_loss 0.1951291561126709\n",
            "Train Epoch: 3 [10560/14166 (74.49209932279909%)]             Loss:0.20385202765464783            R_loss 6.302951812744141, KLD_loss 0.22031331062316895\n",
            "Train Epoch: 3 [10880/14166 (76.74943566591422%)]             Loss:0.20343714952468872            R_loss 6.3324713706970215, KLD_loss 0.1775175780057907\n",
            "Train Epoch: 3 [11200/14166 (79.00677200902935%)]             Loss:0.20341072976589203            R_loss 6.221911907196045, KLD_loss 0.2872314155101776\n",
            "Train Epoch: 3 [11520/14166 (81.26410835214448%)]             Loss:0.20671571791172028            R_loss 6.4025983810424805, KLD_loss 0.2123047262430191\n",
            "Train Epoch: 3 [11840/14166 (83.52144469525959%)]             Loss:0.2157267928123474            R_loss 6.609166145324707, KLD_loss 0.29409146308898926\n",
            "Train Epoch: 3 [12160/14166 (85.77878103837472%)]             Loss:0.20158468186855316            R_loss 6.245867729187012, KLD_loss 0.20484231412410736\n",
            "Train Epoch: 3 [12480/14166 (88.03611738148985%)]             Loss:0.19914793968200684            R_loss 6.147840976715088, KLD_loss 0.2248927503824234\n",
            "Train Epoch: 3 [12800/14166 (90.29345372460497%)]             Loss:0.21212531626224518            R_loss 6.478249549865723, KLD_loss 0.3097601532936096\n",
            "Train Epoch: 3 [13120/14166 (92.55079006772009%)]             Loss:0.20815013349056244            R_loss 6.411328315734863, KLD_loss 0.24947640299797058\n",
            "Train Epoch: 3 [13440/14166 (94.80812641083521%)]             Loss:0.21499675512313843            R_loss 6.579296112060547, KLD_loss 0.30059972405433655\n",
            "Train Epoch: 3 [13760/14166 (97.06546275395034%)]             Loss:0.20379576086997986            R_loss 6.3113603591918945, KLD_loss 0.21010459959506989\n",
            "Train Epoch: 3 [14080/14166 (99.32279909706546%)]             Loss:0.20776242017745972            R_loss 6.381789207458496, KLD_loss 0.26660850644111633\n",
            "====> Epoch: 3 Average loss: 0.2082\n",
            "====> Test set loss: 0.2081\n",
            "Train Epoch: 4 [0/14166 (0.0%)]             Loss:0.2058013677597046            R_loss 6.3372602462768555, KLD_loss 0.24838334321975708\n",
            "Train Epoch: 4 [320/14166 (2.2573363431151243%)]             Loss:0.20750583708286285            R_loss 6.339409351348877, KLD_loss 0.3007773160934448\n",
            "Train Epoch: 4 [640/14166 (4.514672686230249%)]             Loss:0.20693209767341614            R_loss 6.425795078277588, KLD_loss 0.19603195786476135\n",
            "Train Epoch: 4 [960/14166 (6.772009029345372%)]             Loss:0.19956769049167633            R_loss 6.145908832550049, KLD_loss 0.2402573823928833\n",
            "Train Epoch: 4 [1280/14166 (9.029345372460497%)]             Loss:0.2135816514492035            R_loss 6.4775190353393555, KLD_loss 0.3570939004421234\n",
            "Train Epoch: 4 [1600/14166 (11.286681715575622%)]             Loss:0.2089415043592453            R_loss 6.36007833480835, KLD_loss 0.3260496258735657\n",
            "Train Epoch: 4 [1920/14166 (13.544018058690744%)]             Loss:0.19608263671398163            R_loss 6.068964958190918, KLD_loss 0.20567896962165833\n",
            "Train Epoch: 4 [2240/14166 (15.801354401805868%)]             Loss:0.20282863080501556            R_loss 6.280989646911621, KLD_loss 0.20952627062797546\n",
            "Train Epoch: 4 [2560/14166 (18.058690744920995%)]             Loss:0.20905175805091858            R_loss 6.448183536529541, KLD_loss 0.2414722740650177\n",
            "Train Epoch: 4 [2880/14166 (20.31602708803612%)]             Loss:0.21629801392555237            R_loss 6.53373908996582, KLD_loss 0.3877972364425659\n",
            "Train Epoch: 4 [3200/14166 (22.573363431151243%)]             Loss:0.20372849702835083            R_loss 6.22271728515625, KLD_loss 0.29659503698349\n",
            "Train Epoch: 4 [3520/14166 (24.830699774266364%)]             Loss:0.2047068327665329            R_loss 6.332298755645752, KLD_loss 0.2183203101158142\n",
            "Train Epoch: 4 [3840/14166 (27.08803611738149%)]             Loss:0.21993020176887512            R_loss 6.678043365478516, KLD_loss 0.3597223162651062\n",
            "Train Epoch: 4 [4160/14166 (29.345372460496613%)]             Loss:0.21340543031692505            R_loss 6.562620639801025, KLD_loss 0.26635318994522095\n",
            "Train Epoch: 4 [4480/14166 (31.602708803611737%)]             Loss:0.22087644040584564            R_loss 6.702024936676025, KLD_loss 0.3660218119621277\n",
            "Train Epoch: 4 [4800/14166 (33.86004514672686%)]             Loss:0.21614030003547668            R_loss 6.5749101638793945, KLD_loss 0.341579794883728\n",
            "Train Epoch: 4 [5120/14166 (36.11738148984199%)]             Loss:0.21314097940921783            R_loss 6.405466556549072, KLD_loss 0.41504549980163574\n",
            "Train Epoch: 4 [5440/14166 (38.37471783295711%)]             Loss:0.2041112333536148            R_loss 6.285376071929932, KLD_loss 0.24618327617645264\n",
            "Train Epoch: 4 [5760/14166 (40.63205417607224%)]             Loss:0.2046046257019043            R_loss 6.28853178024292, KLD_loss 0.25881582498550415\n",
            "Train Epoch: 4 [6080/14166 (42.88939051918736%)]             Loss:0.208628311753273            R_loss 6.404728889465332, KLD_loss 0.2713768780231476\n",
            "Train Epoch: 4 [6400/14166 (45.146726862302486%)]             Loss:0.20894207060337067            R_loss 6.408481597900391, KLD_loss 0.2776648998260498\n",
            "Train Epoch: 4 [6720/14166 (47.40406320541761%)]             Loss:0.2048158198595047            R_loss 6.286383152008057, KLD_loss 0.2677232027053833\n",
            "Train Epoch: 4 [7040/14166 (49.66139954853273%)]             Loss:0.2083994597196579            R_loss 6.327912330627441, KLD_loss 0.34087058901786804\n",
            "Train Epoch: 4 [7360/14166 (51.918735891647856%)]             Loss:0.199197456240654            R_loss 6.158895969390869, KLD_loss 0.215422585606575\n",
            "Train Epoch: 4 [7680/14166 (54.17607223476298%)]             Loss:0.2055664211511612            R_loss 6.300553321838379, KLD_loss 0.2775719463825226\n",
            "Train Epoch: 4 [8000/14166 (56.433408577878104%)]             Loss:0.2041204869747162            R_loss 6.2956223487854, KLD_loss 0.23623336851596832\n",
            "Train Epoch: 4 [8320/14166 (58.690744920993225%)]             Loss:0.19970732927322388            R_loss 6.223930835723877, KLD_loss 0.16670364141464233\n",
            "Train Epoch: 4 [8640/14166 (60.94808126410835%)]             Loss:0.2042844593524933            R_loss 6.333893299102783, KLD_loss 0.20320941507816315\n",
            "Train Epoch: 4 [8960/14166 (63.205417607223474%)]             Loss:0.21198266744613647            R_loss 6.453915119171143, KLD_loss 0.3295302391052246\n",
            "Train Epoch: 4 [9280/14166 (65.4627539503386%)]             Loss:0.21422402560710907            R_loss 6.526437759399414, KLD_loss 0.32873132824897766\n",
            "Train Epoch: 4 [9600/14166 (67.72009029345372%)]             Loss:0.2003357857465744            R_loss 6.154067039489746, KLD_loss 0.2566779553890228\n",
            "Train Epoch: 4 [9920/14166 (69.97742663656885%)]             Loss:0.20990228652954102            R_loss 6.435210704803467, KLD_loss 0.2816622257232666\n",
            "Train Epoch: 4 [10240/14166 (72.23476297968398%)]             Loss:0.20325079560279846            R_loss 6.204844951629639, KLD_loss 0.2991804778575897\n",
            "Train Epoch: 4 [10560/14166 (74.49209932279909%)]             Loss:0.21563079953193665            R_loss 6.627036094665527, KLD_loss 0.27314916253089905\n",
            "Train Epoch: 4 [10880/14166 (76.74943566591422%)]             Loss:0.20333291590213776            R_loss 6.297269344329834, KLD_loss 0.20938396453857422\n",
            "Train Epoch: 4 [11200/14166 (79.00677200902935%)]             Loss:0.206208273768425            R_loss 6.370393753051758, KLD_loss 0.2282710075378418\n",
            "Train Epoch: 4 [11520/14166 (81.26410835214448%)]             Loss:0.20628246665000916            R_loss 6.412749767303467, KLD_loss 0.1882891058921814\n",
            "Train Epoch: 4 [11840/14166 (83.52144469525959%)]             Loss:0.20542317628860474            R_loss 6.260031700134277, KLD_loss 0.3135097026824951\n",
            "Train Epoch: 4 [12160/14166 (85.77878103837472%)]             Loss:0.21484500169754028            R_loss 6.5972394943237305, KLD_loss 0.27780067920684814\n",
            "Train Epoch: 4 [12480/14166 (88.03611738148985%)]             Loss:0.20421895384788513            R_loss 6.2589616775512695, KLD_loss 0.2760452628135681\n",
            "Train Epoch: 4 [12800/14166 (90.29345372460497%)]             Loss:0.20915450155735016            R_loss 6.45457649230957, KLD_loss 0.23836752772331238\n",
            "Train Epoch: 4 [13120/14166 (92.55079006772009%)]             Loss:0.2064593881368637            R_loss 6.390945911407471, KLD_loss 0.21575458347797394\n",
            "Train Epoch: 4 [13440/14166 (94.80812641083521%)]             Loss:0.20901252329349518            R_loss 6.365461349487305, KLD_loss 0.32293957471847534\n",
            "Train Epoch: 4 [13760/14166 (97.06546275395034%)]             Loss:0.20538127422332764            R_loss 6.316011428833008, KLD_loss 0.2561899423599243\n",
            "Train Epoch: 4 [14080/14166 (99.32279909706546%)]             Loss:0.20479997992515564            R_loss 6.307584762573242, KLD_loss 0.24601423740386963\n",
            "====> Epoch: 4 Average loss: 0.2080\n",
            "====> Test set loss: 0.2081\n",
            "Train Epoch: 5 [0/14166 (0.0%)]             Loss:0.20450927317142487            R_loss 6.23726224899292, KLD_loss 0.30703431367874146\n",
            "Train Epoch: 5 [320/14166 (2.2573363431151243%)]             Loss:0.2139943242073059            R_loss 6.6269121170043945, KLD_loss 0.22090600430965424\n",
            "Train Epoch: 5 [640/14166 (4.514672686230249%)]             Loss:0.21955013275146484            R_loss 6.731200695037842, KLD_loss 0.29440343379974365\n",
            "Train Epoch: 5 [960/14166 (6.772009029345372%)]             Loss:0.2066141813993454            R_loss 6.343606948852539, KLD_loss 0.2680467963218689\n",
            "Train Epoch: 5 [1280/14166 (9.029345372460497%)]             Loss:0.20467549562454224            R_loss 6.2762675285339355, KLD_loss 0.27334845066070557\n",
            "Train Epoch: 5 [1600/14166 (11.286681715575622%)]             Loss:0.21731647849082947            R_loss 6.486109733581543, KLD_loss 0.4680175483226776\n",
            "Train Epoch: 5 [1920/14166 (13.544018058690744%)]             Loss:0.21457070112228394            R_loss 6.587502956390381, KLD_loss 0.27875956892967224\n",
            "Train Epoch: 5 [2240/14166 (15.801354401805868%)]             Loss:0.20634087920188904            R_loss 6.281948089599609, KLD_loss 0.3209603726863861\n",
            "Train Epoch: 5 [2560/14166 (18.058690744920995%)]             Loss:0.2004014104604721            R_loss 6.162349700927734, KLD_loss 0.25049543380737305\n",
            "Train Epoch: 5 [2880/14166 (20.31602708803612%)]             Loss:0.20967887341976166            R_loss 6.468215465545654, KLD_loss 0.24150848388671875\n",
            "Train Epoch: 5 [3200/14166 (22.573363431151243%)]             Loss:0.2069624662399292            R_loss 6.3727874755859375, KLD_loss 0.25001147389411926\n",
            "Train Epoch: 5 [3520/14166 (24.830699774266364%)]             Loss:0.21191464364528656            R_loss 6.465697288513184, KLD_loss 0.31557130813598633\n",
            "Train Epoch: 5 [3840/14166 (27.08803611738149%)]             Loss:0.21994608640670776            R_loss 6.749730110168457, KLD_loss 0.288545161485672\n",
            "Train Epoch: 5 [4160/14166 (29.345372460496613%)]             Loss:0.20654745399951935            R_loss 6.3173627853393555, KLD_loss 0.2921557128429413\n",
            "Train Epoch: 5 [4480/14166 (31.602708803611737%)]             Loss:0.2069651037454605            R_loss 6.4049072265625, KLD_loss 0.21797627210617065\n",
            "Train Epoch: 5 [4800/14166 (33.86004514672686%)]             Loss:0.2110917866230011            R_loss 6.469924449920654, KLD_loss 0.28501230478286743\n",
            "Train Epoch: 5 [5120/14166 (36.11738148984199%)]             Loss:0.20196938514709473            R_loss 6.287964820861816, KLD_loss 0.17505523562431335\n",
            "Train Epoch: 5 [5440/14166 (38.37471783295711%)]             Loss:0.20769155025482178            R_loss 6.390252590179443, KLD_loss 0.2558772563934326\n",
            "Train Epoch: 5 [5760/14166 (40.63205417607224%)]             Loss:0.20498138666152954            R_loss 6.330433368682861, KLD_loss 0.22897067666053772\n",
            "Train Epoch: 5 [6080/14166 (42.88939051918736%)]             Loss:0.20646783709526062            R_loss 6.385402202606201, KLD_loss 0.2215680480003357\n",
            "Train Epoch: 5 [6400/14166 (45.146726862302486%)]             Loss:0.2099740356206894            R_loss 6.440295219421387, KLD_loss 0.2788740396499634\n",
            "Train Epoch: 5 [6720/14166 (47.40406320541761%)]             Loss:0.20919324457645416            R_loss 6.491518974304199, KLD_loss 0.2026648074388504\n",
            "Train Epoch: 5 [7040/14166 (49.66139954853273%)]             Loss:0.20246370136737823            R_loss 6.322367191314697, KLD_loss 0.15647155046463013\n",
            "Train Epoch: 5 [7360/14166 (51.918735891647856%)]             Loss:0.20989513397216797            R_loss 6.381222724914551, KLD_loss 0.33542099595069885\n",
            "Train Epoch: 5 [7680/14166 (54.17607223476298%)]             Loss:0.20600979030132294            R_loss 6.2855305671691895, KLD_loss 0.3067828416824341\n",
            "Train Epoch: 5 [8000/14166 (56.433408577878104%)]             Loss:0.19805468618869781            R_loss 6.122512340545654, KLD_loss 0.21523696184158325\n",
            "Train Epoch: 5 [8320/14166 (58.690744920993225%)]             Loss:0.2076670229434967            R_loss 6.316545009613037, KLD_loss 0.32879993319511414\n",
            "Train Epoch: 5 [8640/14166 (60.94808126410835%)]             Loss:0.1972167193889618            R_loss 6.128426551818848, KLD_loss 0.18250811100006104\n",
            "Train Epoch: 5 [8960/14166 (63.205417607223474%)]             Loss:0.21474610269069672            R_loss 6.607468128204346, KLD_loss 0.26440781354904175\n",
            "Train Epoch: 5 [9280/14166 (65.4627539503386%)]             Loss:0.2114136517047882            R_loss 6.429075241088867, KLD_loss 0.3361615240573883\n",
            "Train Epoch: 5 [9600/14166 (67.72009029345372%)]             Loss:0.21941405534744263            R_loss 6.628436088562012, KLD_loss 0.3928139805793762\n",
            "Train Epoch: 5 [9920/14166 (69.97742663656885%)]             Loss:0.2054491937160492            R_loss 6.347195625305176, KLD_loss 0.2271784245967865\n",
            "Train Epoch: 5 [10240/14166 (72.23476297968398%)]             Loss:0.20931044220924377            R_loss 6.363574981689453, KLD_loss 0.334359347820282\n",
            "Train Epoch: 5 [10560/14166 (74.49209932279909%)]             Loss:0.21217231452465057            R_loss 6.497194290161133, KLD_loss 0.2923201322555542\n",
            "Train Epoch: 5 [10880/14166 (76.74943566591422%)]             Loss:0.2059398740530014            R_loss 6.257013320922852, KLD_loss 0.3330630362033844\n",
            "Train Epoch: 5 [11200/14166 (79.00677200902935%)]             Loss:0.2101747989654541            R_loss 6.552639484405518, KLD_loss 0.17295414209365845\n",
            "Train Epoch: 5 [11520/14166 (81.26410835214448%)]             Loss:0.20957118272781372            R_loss 6.437839508056641, KLD_loss 0.26843854784965515\n",
            "Train Epoch: 5 [11840/14166 (83.52144469525959%)]             Loss:0.20550422370433807            R_loss 6.407482147216797, KLD_loss 0.16865235567092896\n",
            "Train Epoch: 5 [12160/14166 (85.77878103837472%)]             Loss:0.21211829781532288            R_loss 6.456115245819092, KLD_loss 0.3316707909107208\n",
            "Train Epoch: 5 [12480/14166 (88.03611738148985%)]             Loss:0.21569612622261047            R_loss 6.563114643096924, KLD_loss 0.3391609787940979\n",
            "Train Epoch: 5 [12800/14166 (90.29345372460497%)]             Loss:0.19827432930469513            R_loss 6.111119270324707, KLD_loss 0.23365901410579681\n",
            "Train Epoch: 5 [13120/14166 (92.55079006772009%)]             Loss:0.20679999887943268            R_loss 6.419497966766357, KLD_loss 0.19810190796852112\n",
            "Train Epoch: 5 [13440/14166 (94.80812641083521%)]             Loss:0.20989647507667542            R_loss 6.493008613586426, KLD_loss 0.22367829084396362\n",
            "Train Epoch: 5 [13760/14166 (97.06546275395034%)]             Loss:0.21832028031349182            R_loss 6.669985294342041, KLD_loss 0.31626397371292114\n",
            "Train Epoch: 5 [14080/14166 (99.32279909706546%)]             Loss:0.2239890992641449            R_loss 6.824042797088623, KLD_loss 0.3436087965965271\n",
            "====> Epoch: 5 Average loss: 0.2082\n",
            "====> Test set loss: 0.2089\n",
            "Train Epoch: 6 [0/14166 (0.0%)]             Loss:0.20156358182430267            R_loss 6.2612714767456055, KLD_loss 0.18876294791698456\n",
            "Train Epoch: 6 [320/14166 (2.2573363431151243%)]             Loss:0.21177437901496887            R_loss 6.508310794830322, KLD_loss 0.26846933364868164\n",
            "Train Epoch: 6 [640/14166 (4.514672686230249%)]             Loss:0.21358251571655273            R_loss 6.528875350952148, KLD_loss 0.3057652413845062\n",
            "Train Epoch: 6 [960/14166 (6.772009029345372%)]             Loss:0.21511268615722656            R_loss 6.501431465148926, KLD_loss 0.38217446208000183\n",
            "Train Epoch: 6 [1280/14166 (9.029345372460497%)]             Loss:0.21122804284095764            R_loss 6.462585926055908, KLD_loss 0.2967112958431244\n",
            "Train Epoch: 6 [1600/14166 (11.286681715575622%)]             Loss:0.21202653646469116            R_loss 6.533365249633789, KLD_loss 0.2514839172363281\n",
            "Train Epoch: 6 [1920/14166 (13.544018058690744%)]             Loss:0.20502163469791412            R_loss 6.282094955444336, KLD_loss 0.2785976529121399\n",
            "Train Epoch: 6 [2240/14166 (15.801354401805868%)]             Loss:0.19858187437057495            R_loss 6.14194393157959, KLD_loss 0.21267591416835785\n",
            "Train Epoch: 6 [2560/14166 (18.058690744920995%)]             Loss:0.2031843066215515            R_loss 6.222986698150635, KLD_loss 0.27891120314598083\n",
            "Train Epoch: 6 [2880/14166 (20.31602708803612%)]             Loss:0.2136201113462448            R_loss 6.5586466789245605, KLD_loss 0.27719688415527344\n",
            "Train Epoch: 6 [3200/14166 (22.573363431151243%)]             Loss:0.21341794729232788            R_loss 6.450826168060303, KLD_loss 0.3785479664802551\n",
            "Train Epoch: 6 [3520/14166 (24.830699774266364%)]             Loss:0.20578230917453766            R_loss 6.302431583404541, KLD_loss 0.2826021909713745\n",
            "Train Epoch: 6 [3840/14166 (27.08803611738149%)]             Loss:0.21639035642147064            R_loss 6.555529594421387, KLD_loss 0.3689618706703186\n",
            "Train Epoch: 6 [4160/14166 (29.345372460496613%)]             Loss:0.2012423723936081            R_loss 6.244815826416016, KLD_loss 0.19494004547595978\n",
            "Train Epoch: 6 [4480/14166 (31.602708803611737%)]             Loss:0.19523116946220398            R_loss 6.081653118133545, KLD_loss 0.16574451327323914\n",
            "Train Epoch: 6 [4800/14166 (33.86004514672686%)]             Loss:0.20503412187099457            R_loss 6.296689033508301, KLD_loss 0.26440340280532837\n",
            "Train Epoch: 6 [5120/14166 (36.11738148984199%)]             Loss:0.21449878811836243            R_loss 6.616477012634277, KLD_loss 0.2474842667579651\n",
            "Train Epoch: 6 [5440/14166 (38.37471783295711%)]             Loss:0.21303752064704895            R_loss 6.479427337646484, KLD_loss 0.33777275681495667\n",
            "Train Epoch: 6 [5760/14166 (40.63205417607224%)]             Loss:0.20523570477962494            R_loss 6.255285739898682, KLD_loss 0.312257319688797\n",
            "Train Epoch: 6 [6080/14166 (42.88939051918736%)]             Loss:0.19788865745067596            R_loss 6.120236396789551, KLD_loss 0.21220070123672485\n",
            "Train Epoch: 6 [6400/14166 (45.146726862302486%)]             Loss:0.20809222757816315            R_loss 6.285937786102295, KLD_loss 0.3730136752128601\n",
            "Train Epoch: 6 [6720/14166 (47.40406320541761%)]             Loss:0.20335420966148376            R_loss 6.3158063888549805, KLD_loss 0.19152842462062836\n",
            "Train Epoch: 6 [7040/14166 (49.66139954853273%)]             Loss:0.2056402862071991            R_loss 6.354973793029785, KLD_loss 0.22551597654819489\n",
            "Train Epoch: 6 [7360/14166 (51.918735891647856%)]             Loss:0.21584556996822357            R_loss 6.589347839355469, KLD_loss 0.31771042943000793\n",
            "Train Epoch: 6 [7680/14166 (54.17607223476298%)]             Loss:0.19508230686187744            R_loss 6.073943614959717, KLD_loss 0.1686902642250061\n",
            "Train Epoch: 6 [8000/14166 (56.433408577878104%)]             Loss:0.21265600621700287            R_loss 6.482522010803223, KLD_loss 0.32246968150138855\n",
            "Train Epoch: 6 [8320/14166 (58.690744920993225%)]             Loss:0.21093115210533142            R_loss 6.504871845245361, KLD_loss 0.24492479860782623\n",
            "Train Epoch: 6 [8640/14166 (60.94808126410835%)]             Loss:0.2096298187971115            R_loss 6.476128578186035, KLD_loss 0.23202583193778992\n",
            "Train Epoch: 6 [8960/14166 (63.205417607223474%)]             Loss:0.2084151804447174            R_loss 6.442638397216797, KLD_loss 0.22664687037467957\n",
            "Train Epoch: 6 [9280/14166 (65.4627539503386%)]             Loss:0.21161030232906342            R_loss 6.447752475738525, KLD_loss 0.32377731800079346\n",
            "Train Epoch: 6 [9600/14166 (67.72009029345372%)]             Loss:0.20496000349521637            R_loss 6.2615766525268555, KLD_loss 0.29714345932006836\n",
            "Train Epoch: 6 [9920/14166 (69.97742663656885%)]             Loss:0.20642994344234467            R_loss 6.365467071533203, KLD_loss 0.24029116332530975\n",
            "Train Epoch: 6 [10240/14166 (72.23476297968398%)]             Loss:0.20472443103790283            R_loss 6.305041313171387, KLD_loss 0.2461402267217636\n",
            "Train Epoch: 6 [10560/14166 (74.49209932279909%)]             Loss:0.2058672159910202            R_loss 6.227365016937256, KLD_loss 0.3603858947753906\n",
            "Train Epoch: 6 [10880/14166 (76.74943566591422%)]             Loss:0.20423224568367004            R_loss 6.220775604248047, KLD_loss 0.3146568536758423\n",
            "Train Epoch: 6 [11200/14166 (79.00677200902935%)]             Loss:0.2154253125190735            R_loss 6.654789447784424, KLD_loss 0.23882034420967102\n",
            "Train Epoch: 6 [11520/14166 (81.26410835214448%)]             Loss:0.21283920109272003            R_loss 6.3980393409729, KLD_loss 0.41281554102897644\n",
            "Train Epoch: 6 [11840/14166 (83.52144469525959%)]             Loss:0.20955154299736023            R_loss 6.401623725891113, KLD_loss 0.3040257394313812\n",
            "Train Epoch: 6 [12160/14166 (85.77878103837472%)]             Loss:0.20203767716884613            R_loss 6.224878787994385, KLD_loss 0.2403266876935959\n",
            "Train Epoch: 6 [12480/14166 (88.03611738148985%)]             Loss:0.21078382432460785            R_loss 6.459778785705566, KLD_loss 0.2853037714958191\n",
            "Train Epoch: 6 [12800/14166 (90.29345372460497%)]             Loss:0.21540690958499908            R_loss 6.483190059661865, KLD_loss 0.4098312556743622\n",
            "Train Epoch: 6 [13120/14166 (92.55079006772009%)]             Loss:0.19244207441806793            R_loss 6.020991325378418, KLD_loss 0.1371547281742096\n",
            "Train Epoch: 6 [13440/14166 (94.80812641083521%)]             Loss:0.20990146696567535            R_loss 6.524850845336914, KLD_loss 0.19199611246585846\n",
            "Train Epoch: 6 [13760/14166 (97.06546275395034%)]             Loss:0.2047489732503891            R_loss 6.2802734375, KLD_loss 0.2716944217681885\n",
            "Train Epoch: 6 [14080/14166 (99.32279909706546%)]             Loss:0.20385000109672546            R_loss 6.305094242095947, KLD_loss 0.21810582280158997\n",
            "====> Epoch: 6 Average loss: 0.2079\n",
            "====> Test set loss: 0.2076\n",
            "Train Epoch: 7 [0/14166 (0.0%)]             Loss:0.2251887172460556            R_loss 6.8819074630737305, KLD_loss 0.32413148880004883\n",
            "Train Epoch: 7 [320/14166 (2.2573363431151243%)]             Loss:0.22097843885421753            R_loss 6.7463555335998535, KLD_loss 0.32495439052581787\n",
            "Train Epoch: 7 [640/14166 (4.514672686230249%)]             Loss:0.20094135403633118            R_loss 6.157027244567871, KLD_loss 0.2730962634086609\n",
            "Train Epoch: 7 [960/14166 (6.772009029345372%)]             Loss:0.20150411128997803            R_loss 6.162049293518066, KLD_loss 0.2860821485519409\n",
            "Train Epoch: 7 [1280/14166 (9.029345372460497%)]             Loss:0.21768924593925476            R_loss 6.69157600402832, KLD_loss 0.27448034286499023\n",
            "Train Epoch: 7 [1600/14166 (11.286681715575622%)]             Loss:0.2084491103887558            R_loss 6.370856761932373, KLD_loss 0.29951488971710205\n",
            "Train Epoch: 7 [1920/14166 (13.544018058690744%)]             Loss:0.19967585802078247            R_loss 6.13060188293457, KLD_loss 0.2590252161026001\n",
            "Train Epoch: 7 [2240/14166 (15.801354401805868%)]             Loss:0.21601474285125732            R_loss 6.561619758605957, KLD_loss 0.350851833820343\n",
            "Train Epoch: 7 [2560/14166 (18.058690744920995%)]             Loss:0.23121345043182373            R_loss 6.927954196929932, KLD_loss 0.4708765149116516\n",
            "Train Epoch: 7 [2880/14166 (20.31602708803612%)]             Loss:0.21238955855369568            R_loss 6.612813949584961, KLD_loss 0.18365156650543213\n",
            "Train Epoch: 7 [3200/14166 (22.573363431151243%)]             Loss:0.20582175254821777            R_loss 6.336341381072998, KLD_loss 0.24995408952236176\n",
            "Train Epoch: 7 [3520/14166 (24.830699774266364%)]             Loss:0.21712981164455414            R_loss 6.661900997161865, KLD_loss 0.28625261783599854\n",
            "Train Epoch: 7 [3840/14166 (27.08803611738149%)]             Loss:0.20192758738994598            R_loss 6.164388656616211, KLD_loss 0.29729434847831726\n",
            "Train Epoch: 7 [4160/14166 (29.345372460496613%)]             Loss:0.20733089745044708            R_loss 6.3231306076049805, KLD_loss 0.31145837903022766\n",
            "Train Epoch: 7 [4480/14166 (31.602708803611737%)]             Loss:0.2056684046983719            R_loss 6.290309429168701, KLD_loss 0.2910791039466858\n",
            "Train Epoch: 7 [4800/14166 (33.86004514672686%)]             Loss:0.21690097451210022            R_loss 6.618135452270508, KLD_loss 0.32269570231437683\n",
            "Train Epoch: 7 [5120/14166 (36.11738148984199%)]             Loss:0.21424633264541626            R_loss 6.581591606140137, KLD_loss 0.2742912769317627\n",
            "Train Epoch: 7 [5440/14166 (38.37471783295711%)]             Loss:0.20385563373565674            R_loss 6.265681266784668, KLD_loss 0.25769880414009094\n",
            "Train Epoch: 7 [5760/14166 (40.63205417607224%)]             Loss:0.19396619498729706            R_loss 6.056258201599121, KLD_loss 0.15066000819206238\n",
            "Train Epoch: 7 [6080/14166 (42.88939051918736%)]             Loss:0.21123987436294556            R_loss 6.390303134918213, KLD_loss 0.36937248706817627\n",
            "Train Epoch: 7 [6400/14166 (45.146726862302486%)]             Loss:0.20763641595840454            R_loss 6.339506149291992, KLD_loss 0.3048590123653412\n",
            "Train Epoch: 7 [6720/14166 (47.40406320541761%)]             Loss:0.21251532435417175            R_loss 6.532817840576172, KLD_loss 0.2676728367805481\n",
            "Train Epoch: 7 [7040/14166 (49.66139954853273%)]             Loss:0.22260600328445435            R_loss 6.723093032836914, KLD_loss 0.4002991020679474\n",
            "Train Epoch: 7 [7360/14166 (51.918735891647856%)]             Loss:0.22512269020080566            R_loss 6.898890018463135, KLD_loss 0.30503618717193604\n",
            "Train Epoch: 7 [7680/14166 (54.17607223476298%)]             Loss:0.20149165391921997            R_loss 6.220349311828613, KLD_loss 0.2273835390806198\n",
            "Train Epoch: 7 [8000/14166 (56.433408577878104%)]             Loss:0.2087416797876358            R_loss 6.449572563171387, KLD_loss 0.23016072809696198\n",
            "Train Epoch: 7 [8320/14166 (58.690744920993225%)]             Loss:0.1991310864686966            R_loss 6.172683238983154, KLD_loss 0.19951146841049194\n",
            "Train Epoch: 7 [8640/14166 (60.94808126410835%)]             Loss:0.20625780522823334            R_loss 6.3353962898254395, KLD_loss 0.26485323905944824\n",
            "Train Epoch: 7 [8960/14166 (63.205417607223474%)]             Loss:0.20223011076450348            R_loss 6.179263114929199, KLD_loss 0.29210013151168823\n",
            "Train Epoch: 7 [9280/14166 (65.4627539503386%)]             Loss:0.2079695761203766            R_loss 6.285957336425781, KLD_loss 0.36906924843788147\n",
            "Train Epoch: 7 [9600/14166 (67.72009029345372%)]             Loss:0.21049568057060242            R_loss 6.489222526550293, KLD_loss 0.24664007127285004\n",
            "Train Epoch: 7 [9920/14166 (69.97742663656885%)]             Loss:0.21324560046195984            R_loss 6.490860462188721, KLD_loss 0.33299878239631653\n",
            "Train Epoch: 7 [10240/14166 (72.23476297968398%)]             Loss:0.20394083857536316            R_loss 6.332757949829102, KLD_loss 0.19334907829761505\n",
            "Train Epoch: 7 [10560/14166 (74.49209932279909%)]             Loss:0.20898258686065674            R_loss 6.4402546882629395, KLD_loss 0.2471882551908493\n",
            "Train Epoch: 7 [10880/14166 (76.74943566591422%)]             Loss:0.21125935018062592            R_loss 6.515856742858887, KLD_loss 0.24444270133972168\n",
            "Train Epoch: 7 [11200/14166 (79.00677200902935%)]             Loss:0.19840267300605774            R_loss 6.146995544433594, KLD_loss 0.2018895149230957\n",
            "Train Epoch: 7 [11520/14166 (81.26410835214448%)]             Loss:0.21579276025295258            R_loss 6.668193340301514, KLD_loss 0.23717544972896576\n",
            "Train Epoch: 7 [11840/14166 (83.52144469525959%)]             Loss:0.20682471990585327            R_loss 6.287117004394531, KLD_loss 0.3312745988368988\n",
            "Train Epoch: 7 [12160/14166 (85.77878103837472%)]             Loss:0.21930718421936035            R_loss 6.636410713195801, KLD_loss 0.38141903281211853\n",
            "Train Epoch: 7 [12480/14166 (88.03611738148985%)]             Loss:0.20980390906333923            R_loss 6.4405598640441895, KLD_loss 0.2731654644012451\n",
            "Train Epoch: 7 [12800/14166 (90.29345372460497%)]             Loss:0.20324952900409698            R_loss 6.268759727478027, KLD_loss 0.23522527515888214\n",
            "Train Epoch: 7 [13120/14166 (92.55079006772009%)]             Loss:0.20460699498653412            R_loss 6.3040242195129395, KLD_loss 0.2433997094631195\n",
            "Train Epoch: 7 [13440/14166 (94.80812641083521%)]             Loss:0.20768451690673828            R_loss 6.305426120758057, KLD_loss 0.3404783606529236\n",
            "Train Epoch: 7 [13760/14166 (97.06546275395034%)]             Loss:0.20031599700450897            R_loss 6.171145915985107, KLD_loss 0.2389664202928543\n",
            "Train Epoch: 7 [14080/14166 (99.32279909706546%)]             Loss:0.2139105647802353            R_loss 6.4606170654296875, KLD_loss 0.3845214247703552\n",
            "====> Epoch: 7 Average loss: 0.2080\n",
            "====> Test set loss: 0.2076\n",
            "Train Epoch: 8 [0/14166 (0.0%)]             Loss:0.21443186700344086            R_loss 6.6147918701171875, KLD_loss 0.2470281422138214\n",
            "Train Epoch: 8 [320/14166 (2.2573363431151243%)]             Loss:0.2033105045557022            R_loss 6.234519004821777, KLD_loss 0.27141645550727844\n",
            "Train Epoch: 8 [640/14166 (4.514672686230249%)]             Loss:0.21126830577850342            R_loss 6.438414573669434, KLD_loss 0.3221711814403534\n",
            "Train Epoch: 8 [960/14166 (6.772009029345372%)]             Loss:0.20508353412151337            R_loss 6.333885192871094, KLD_loss 0.22878816723823547\n",
            "Train Epoch: 8 [1280/14166 (9.029345372460497%)]             Loss:0.2083125114440918            R_loss 6.419174671173096, KLD_loss 0.2468254268169403\n",
            "Train Epoch: 8 [1600/14166 (11.286681715575622%)]             Loss:0.19831763207912445            R_loss 6.172301292419434, KLD_loss 0.17386358976364136\n",
            "Train Epoch: 8 [1920/14166 (13.544018058690744%)]             Loss:0.20612704753875732            R_loss 6.355358123779297, KLD_loss 0.2407076358795166\n",
            "Train Epoch: 8 [2240/14166 (15.801354401805868%)]             Loss:0.20994442701339722            R_loss 6.406397819519043, KLD_loss 0.31182414293289185\n",
            "Train Epoch: 8 [2560/14166 (18.058690744920995%)]             Loss:0.20340241491794586            R_loss 6.271520137786865, KLD_loss 0.23735728859901428\n",
            "Train Epoch: 8 [2880/14166 (20.31602708803612%)]             Loss:0.20970720052719116            R_loss 6.416682243347168, KLD_loss 0.2939484417438507\n",
            "Train Epoch: 8 [3200/14166 (22.573363431151243%)]             Loss:0.20557452738285065            R_loss 6.352169513702393, KLD_loss 0.22621560096740723\n",
            "Train Epoch: 8 [3520/14166 (24.830699774266364%)]             Loss:0.20950771868228912            R_loss 6.37592077255249, KLD_loss 0.32832571864128113\n",
            "Train Epoch: 8 [3840/14166 (27.08803611738149%)]             Loss:0.20426008105278015            R_loss 6.247658729553223, KLD_loss 0.28866374492645264\n",
            "Train Epoch: 8 [4160/14166 (29.345372460496613%)]             Loss:0.22095909714698792            R_loss 6.6667304039001465, KLD_loss 0.40396052598953247\n",
            "Train Epoch: 8 [4480/14166 (31.602708803611737%)]             Loss:0.20385165512561798            R_loss 6.244065284729004, KLD_loss 0.27918770909309387\n",
            "Train Epoch: 8 [4800/14166 (33.86004514672686%)]             Loss:0.21045950055122375            R_loss 6.447516918182373, KLD_loss 0.28718680143356323\n",
            "Train Epoch: 8 [5120/14166 (36.11738148984199%)]             Loss:0.21239304542541504            R_loss 6.516274452209473, KLD_loss 0.2803029417991638\n",
            "Train Epoch: 8 [5440/14166 (38.37471783295711%)]             Loss:0.2063567042350769            R_loss 6.4108686447143555, KLD_loss 0.1925455778837204\n",
            "Train Epoch: 8 [5760/14166 (40.63205417607224%)]             Loss:0.2114347517490387            R_loss 6.474852561950684, KLD_loss 0.2910597026348114\n",
            "Train Epoch: 8 [6080/14166 (42.88939051918736%)]             Loss:0.20777300000190735            R_loss 6.459044456481934, KLD_loss 0.18969137966632843\n",
            "Train Epoch: 8 [6400/14166 (45.146726862302486%)]             Loss:0.20293502509593964            R_loss 6.201831340789795, KLD_loss 0.2920895218849182\n",
            "Train Epoch: 8 [6720/14166 (47.40406320541761%)]             Loss:0.20455224812030792            R_loss 6.2662858963012695, KLD_loss 0.2793857455253601\n",
            "Train Epoch: 8 [7040/14166 (49.66139954853273%)]             Loss:0.20006413757801056            R_loss 6.158370018005371, KLD_loss 0.24368229508399963\n",
            "Train Epoch: 8 [7360/14166 (51.918735891647856%)]             Loss:0.1974775493144989            R_loss 6.144684791564941, KLD_loss 0.17459653317928314\n",
            "Train Epoch: 8 [7680/14166 (54.17607223476298%)]             Loss:0.20584022998809814            R_loss 6.328775882720947, KLD_loss 0.2581116259098053\n",
            "Train Epoch: 8 [8000/14166 (56.433408577878104%)]             Loss:0.20269206166267395            R_loss 6.3216071128845215, KLD_loss 0.16453921794891357\n",
            "Train Epoch: 8 [8320/14166 (58.690744920993225%)]             Loss:0.21533694863319397            R_loss 6.541867733001709, KLD_loss 0.34891462326049805\n",
            "Train Epoch: 8 [8640/14166 (60.94808126410835%)]             Loss:0.21278730034828186            R_loss 6.475039005279541, KLD_loss 0.33415406942367554\n",
            "Train Epoch: 8 [8960/14166 (63.205417607223474%)]             Loss:0.21390452980995178            R_loss 6.454421043395996, KLD_loss 0.39052414894104004\n",
            "Train Epoch: 8 [9280/14166 (65.4627539503386%)]             Loss:0.1995077133178711            R_loss 6.109064102172852, KLD_loss 0.2751834988594055\n",
            "Train Epoch: 8 [9600/14166 (67.72009029345372%)]             Loss:0.20569537580013275            R_loss 6.348156929016113, KLD_loss 0.23409532010555267\n",
            "Train Epoch: 8 [9920/14166 (69.97742663656885%)]             Loss:0.2043299674987793            R_loss 6.302737236022949, KLD_loss 0.23582197725772858\n",
            "Train Epoch: 8 [10240/14166 (72.23476297968398%)]             Loss:0.2154495120048523            R_loss 6.615372657775879, KLD_loss 0.2790118455886841\n",
            "Train Epoch: 8 [10560/14166 (74.49209932279909%)]             Loss:0.19947053492069244            R_loss 6.1408772468566895, KLD_loss 0.24218013882637024\n",
            "Train Epoch: 8 [10880/14166 (76.74943566591422%)]             Loss:0.20478704571723938            R_loss 6.2400360107421875, KLD_loss 0.31314992904663086\n",
            "Train Epoch: 8 [11200/14166 (79.00677200902935%)]             Loss:0.21775488555431366            R_loss 6.66646671295166, KLD_loss 0.3016890287399292\n",
            "Train Epoch: 8 [11520/14166 (81.26410835214448%)]             Loss:0.21427857875823975            R_loss 6.569355010986328, KLD_loss 0.2875592112541199\n",
            "Train Epoch: 8 [11840/14166 (83.52144469525959%)]             Loss:0.2097136527299881            R_loss 6.457109451293945, KLD_loss 0.25372743606567383\n",
            "Train Epoch: 8 [12160/14166 (85.77878103837472%)]             Loss:0.20345538854599            R_loss 6.266204833984375, KLD_loss 0.24436774849891663\n",
            "Train Epoch: 8 [12480/14166 (88.03611738148985%)]             Loss:0.21685820817947388            R_loss 6.532284259796143, KLD_loss 0.4071785807609558\n",
            "Train Epoch: 8 [12800/14166 (90.29345372460497%)]             Loss:0.1993180811405182            R_loss 6.157243728637695, KLD_loss 0.22093451023101807\n",
            "Train Epoch: 8 [13120/14166 (92.55079006772009%)]             Loss:0.21734397113323212            R_loss 6.667176723480225, KLD_loss 0.28783026337623596\n",
            "Train Epoch: 8 [13440/14166 (94.80812641083521%)]             Loss:0.21136273443698883            R_loss 6.40826416015625, KLD_loss 0.3553433418273926\n",
            "Train Epoch: 8 [13760/14166 (97.06546275395034%)]             Loss:0.21751099824905396            R_loss 6.648763656616211, KLD_loss 0.31158873438835144\n",
            "Train Epoch: 8 [14080/14166 (99.32279909706546%)]             Loss:0.20380766689777374            R_loss 6.248307228088379, KLD_loss 0.27353811264038086\n",
            "====> Epoch: 8 Average loss: 0.2081\n",
            "====> Test set loss: 0.2082\n",
            "Train Epoch: 9 [0/14166 (0.0%)]             Loss:0.213434636592865            R_loss 6.565549850463867, KLD_loss 0.2643584609031677\n",
            "Train Epoch: 9 [320/14166 (2.2573363431151243%)]             Loss:0.21231277287006378            R_loss 6.570521354675293, KLD_loss 0.22348666191101074\n",
            "Train Epoch: 9 [640/14166 (4.514672686230249%)]             Loss:0.20497053861618042            R_loss 6.239911079406738, KLD_loss 0.31914612650871277\n",
            "Train Epoch: 9 [960/14166 (6.772009029345372%)]             Loss:0.2096988409757614            R_loss 6.481136798858643, KLD_loss 0.22922569513320923\n",
            "Train Epoch: 9 [1280/14166 (9.029345372460497%)]             Loss:0.20564325153827667            R_loss 6.278238296508789, KLD_loss 0.3023452162742615\n",
            "Train Epoch: 9 [1600/14166 (11.286681715575622%)]             Loss:0.20574307441711426            R_loss 6.32795524597168, KLD_loss 0.2558225393295288\n",
            "Train Epoch: 9 [1920/14166 (13.544018058690744%)]             Loss:0.21470096707344055            R_loss 6.634586334228516, KLD_loss 0.23584479093551636\n",
            "Train Epoch: 9 [2240/14166 (15.801354401805868%)]             Loss:0.2149689793586731            R_loss 6.492352485656738, KLD_loss 0.3866546154022217\n",
            "Train Epoch: 9 [2560/14166 (18.058690744920995%)]             Loss:0.21055001020431519            R_loss 6.458470344543457, KLD_loss 0.2791297733783722\n",
            "Train Epoch: 9 [2880/14166 (20.31602708803612%)]             Loss:0.20897868275642395            R_loss 6.4891357421875, KLD_loss 0.19818182289600372\n",
            "Train Epoch: 9 [3200/14166 (22.573363431151243%)]             Loss:0.20439212024211884            R_loss 6.286546230316162, KLD_loss 0.25400200486183167\n",
            "Train Epoch: 9 [3520/14166 (24.830699774266364%)]             Loss:0.2193128764629364            R_loss 6.674326419830322, KLD_loss 0.3436852991580963\n",
            "Train Epoch: 9 [3840/14166 (27.08803611738149%)]             Loss:0.2099076807498932            R_loss 6.443984031677246, KLD_loss 0.2730620503425598\n",
            "Train Epoch: 9 [4160/14166 (29.345372460496613%)]             Loss:0.19854167103767395            R_loss 6.17339563369751, KLD_loss 0.17993731796741486\n",
            "Train Epoch: 9 [4480/14166 (31.602708803611737%)]             Loss:0.21382158994674683            R_loss 6.490860939025879, KLD_loss 0.35142982006073\n",
            "Train Epoch: 9 [4800/14166 (33.86004514672686%)]             Loss:0.2092972695827484            R_loss 6.375436782836914, KLD_loss 0.3220759928226471\n",
            "Train Epoch: 9 [5120/14166 (36.11738148984199%)]             Loss:0.20251835882663727            R_loss 6.291367530822754, KLD_loss 0.18921992182731628\n",
            "Train Epoch: 9 [5440/14166 (38.37471783295711%)]             Loss:0.20532257854938507            R_loss 6.362730979919434, KLD_loss 0.20759214460849762\n",
            "Train Epoch: 9 [5760/14166 (40.63205417607224%)]             Loss:0.20942670106887817            R_loss 6.443675994873047, KLD_loss 0.25797829031944275\n",
            "Train Epoch: 9 [6080/14166 (42.88939051918736%)]             Loss:0.20783299207687378            R_loss 6.381222724914551, KLD_loss 0.26943349838256836\n",
            "Train Epoch: 9 [6400/14166 (45.146726862302486%)]             Loss:0.2071375697851181            R_loss 6.347504615783691, KLD_loss 0.2808969020843506\n",
            "Train Epoch: 9 [6720/14166 (47.40406320541761%)]             Loss:0.20688366889953613            R_loss 6.371218204498291, KLD_loss 0.24905934929847717\n",
            "Train Epoch: 9 [7040/14166 (49.66139954853273%)]             Loss:0.20123666524887085            R_loss 6.2012248039245605, KLD_loss 0.23834852874279022\n",
            "Train Epoch: 9 [7360/14166 (51.918735891647856%)]             Loss:0.20688818395137787            R_loss 6.346218585968018, KLD_loss 0.2742033898830414\n",
            "Train Epoch: 9 [7680/14166 (54.17607223476298%)]             Loss:0.20701009035110474            R_loss 6.315150260925293, KLD_loss 0.3091723322868347\n",
            "Train Epoch: 9 [8000/14166 (56.433408577878104%)]             Loss:0.20818470418453217            R_loss 6.484239101409912, KLD_loss 0.1776713877916336\n",
            "Train Epoch: 9 [8320/14166 (58.690744920993225%)]             Loss:0.2137027382850647            R_loss 6.573272705078125, KLD_loss 0.26521414518356323\n",
            "Train Epoch: 9 [8640/14166 (60.94808126410835%)]             Loss:0.20744675397872925            R_loss 6.348768711090088, KLD_loss 0.28952744603157043\n",
            "Train Epoch: 9 [8960/14166 (63.205417607223474%)]             Loss:0.21809114515781403            R_loss 6.551749229431152, KLD_loss 0.4271675944328308\n",
            "Train Epoch: 9 [9280/14166 (65.4627539503386%)]             Loss:0.21955598890781403            R_loss 6.651731014251709, KLD_loss 0.37406083941459656\n",
            "Train Epoch: 9 [9600/14166 (67.72009029345372%)]             Loss:0.19575147330760956            R_loss 6.066398620605469, KLD_loss 0.19764810800552368\n",
            "Train Epoch: 9 [9920/14166 (69.97742663656885%)]             Loss:0.21122142672538757            R_loss 6.460762023925781, KLD_loss 0.29832401871681213\n",
            "Train Epoch: 9 [10240/14166 (72.23476297968398%)]             Loss:0.211792454123497            R_loss 6.379216194152832, KLD_loss 0.3981423079967499\n",
            "Train Epoch: 9 [10560/14166 (74.49209932279909%)]             Loss:0.19494931399822235            R_loss 6.087571144104004, KLD_loss 0.15080690383911133\n",
            "Train Epoch: 9 [10880/14166 (76.74943566591422%)]             Loss:0.19674386084079742            R_loss 6.120469093322754, KLD_loss 0.1753346025943756\n",
            "Train Epoch: 9 [11200/14166 (79.00677200902935%)]             Loss:0.20830130577087402            R_loss 6.336260795593262, KLD_loss 0.3293808102607727\n",
            "Train Epoch: 9 [11520/14166 (81.26410835214448%)]             Loss:0.21751612424850464            R_loss 6.601941108703613, KLD_loss 0.3585747182369232\n",
            "Train Epoch: 9 [11840/14166 (83.52144469525959%)]             Loss:0.20655210316181183            R_loss 6.34489107131958, KLD_loss 0.26477646827697754\n",
            "Train Epoch: 9 [12160/14166 (85.77878103837472%)]             Loss:0.21805427968502045            R_loss 6.724001884460449, KLD_loss 0.2537349760532379\n",
            "Train Epoch: 9 [12480/14166 (88.03611738148985%)]             Loss:0.19870759546756744            R_loss 6.147861003875732, KLD_loss 0.21078181266784668\n",
            "Train Epoch: 9 [12800/14166 (90.29345372460497%)]             Loss:0.2013424038887024            R_loss 6.190874099731445, KLD_loss 0.2520829141139984\n",
            "Train Epoch: 9 [13120/14166 (92.55079006772009%)]             Loss:0.20651128888130188            R_loss 6.389321804046631, KLD_loss 0.2190397083759308\n",
            "Train Epoch: 9 [13440/14166 (94.80812641083521%)]             Loss:0.20774486660957336            R_loss 6.299842357635498, KLD_loss 0.34799304604530334\n",
            "Train Epoch: 9 [13760/14166 (97.06546275395034%)]             Loss:0.2129548341035843            R_loss 6.5532121658325195, KLD_loss 0.26134318113327026\n",
            "Train Epoch: 9 [14080/14166 (99.32279909706546%)]             Loss:0.20053453743457794            R_loss 6.208262920379639, KLD_loss 0.2088424563407898\n",
            "====> Epoch: 9 Average loss: 0.2081\n",
            "====> Test set loss: 0.2084\n",
            "Train Epoch: 10 [0/14166 (0.0%)]             Loss:0.19803819060325623            R_loss 6.137263298034668, KLD_loss 0.19995810091495514\n",
            "Train Epoch: 10 [320/14166 (2.2573363431151243%)]             Loss:0.217219740152359            R_loss 6.72138786315918, KLD_loss 0.2296437919139862\n",
            "Train Epoch: 10 [640/14166 (4.514672686230249%)]             Loss:0.2166370451450348            R_loss 6.604494094848633, KLD_loss 0.3278915584087372\n",
            "Train Epoch: 10 [960/14166 (6.772009029345372%)]             Loss:0.21494188904762268            R_loss 6.598987102508545, KLD_loss 0.27915331721305847\n",
            "Train Epoch: 10 [1280/14166 (9.029345372460497%)]             Loss:0.21028293669223785            R_loss 6.405666351318359, KLD_loss 0.32338738441467285\n",
            "Train Epoch: 10 [1600/14166 (11.286681715575622%)]             Loss:0.21850715577602386            R_loss 6.671939849853516, KLD_loss 0.32028907537460327\n",
            "Train Epoch: 10 [1920/14166 (13.544018058690744%)]             Loss:0.21600468456745148            R_loss 6.668741226196289, KLD_loss 0.24340878427028656\n",
            "Train Epoch: 10 [2240/14166 (15.801354401805868%)]             Loss:0.20850367844104767            R_loss 6.480434417724609, KLD_loss 0.19168350100517273\n",
            "Train Epoch: 10 [2560/14166 (18.058690744920995%)]             Loss:0.20908862352371216            R_loss 6.380888938903809, KLD_loss 0.30994656682014465\n",
            "Train Epoch: 10 [2880/14166 (20.31602708803612%)]             Loss:0.20626704394817352            R_loss 6.358199596405029, KLD_loss 0.24234560132026672\n",
            "Train Epoch: 10 [3200/14166 (22.573363431151243%)]             Loss:0.21792708337306976            R_loss 6.690802574157715, KLD_loss 0.28286391496658325\n",
            "Train Epoch: 10 [3520/14166 (24.830699774266364%)]             Loss:0.21061541140079498            R_loss 6.432432174682617, KLD_loss 0.30726146697998047\n",
            "Train Epoch: 10 [3840/14166 (27.08803611738149%)]             Loss:0.2081499844789505            R_loss 6.411694526672363, KLD_loss 0.24910509586334229\n",
            "Train Epoch: 10 [4160/14166 (29.345372460496613%)]             Loss:0.20884212851524353            R_loss 6.41575288772583, KLD_loss 0.267194539308548\n",
            "Train Epoch: 10 [4480/14166 (31.602708803611737%)]             Loss:0.20450006425380707            R_loss 6.350272178649902, KLD_loss 0.19372983276844025\n",
            "Train Epoch: 10 [4800/14166 (33.86004514672686%)]             Loss:0.20223675668239594            R_loss 6.190756320953369, KLD_loss 0.28082039952278137\n",
            "Train Epoch: 10 [5120/14166 (36.11738148984199%)]             Loss:0.2090422511100769            R_loss 6.435150623321533, KLD_loss 0.2542014718055725\n",
            "Train Epoch: 10 [5440/14166 (38.37471783295711%)]             Loss:0.21273130178451538            R_loss 6.468332290649414, KLD_loss 0.3390690088272095\n",
            "Train Epoch: 10 [5760/14166 (40.63205417607224%)]             Loss:0.20374390482902527            R_loss 6.229724884033203, KLD_loss 0.2900799512863159\n",
            "Train Epoch: 10 [6080/14166 (42.88939051918736%)]             Loss:0.21314339339733124            R_loss 6.543324947357178, KLD_loss 0.2772630453109741\n",
            "Train Epoch: 10 [6400/14166 (45.146726862302486%)]             Loss:0.2094922512769699            R_loss 6.4499335289001465, KLD_loss 0.2538187503814697\n",
            "Train Epoch: 10 [6720/14166 (47.40406320541761%)]             Loss:0.21595892310142517            R_loss 6.610082149505615, KLD_loss 0.300603449344635\n",
            "Train Epoch: 10 [7040/14166 (49.66139954853273%)]             Loss:0.21477127075195312            R_loss 6.5375847816467285, KLD_loss 0.33509549498558044\n",
            "Train Epoch: 10 [7360/14166 (51.918735891647856%)]             Loss:0.2072904258966446            R_loss 6.442221641540527, KLD_loss 0.191071555018425\n",
            "Train Epoch: 10 [7680/14166 (54.17607223476298%)]             Loss:0.19667384028434753            R_loss 6.072116851806641, KLD_loss 0.22144603729248047\n",
            "Train Epoch: 10 [8000/14166 (56.433408577878104%)]             Loss:0.20887751877307892            R_loss 6.388521194458008, KLD_loss 0.29555949568748474\n",
            "Train Epoch: 10 [8320/14166 (58.690744920993225%)]             Loss:0.20155303180217743            R_loss 6.249871730804443, KLD_loss 0.19982540607452393\n",
            "Train Epoch: 10 [8640/14166 (60.94808126410835%)]             Loss:0.19861763715744019            R_loss 6.147414684295654, KLD_loss 0.2083497941493988\n",
            "Train Epoch: 10 [8960/14166 (63.205417607223474%)]             Loss:0.21227630972862244            R_loss 6.527076721191406, KLD_loss 0.2657651901245117\n",
            "Train Epoch: 10 [9280/14166 (65.4627539503386%)]             Loss:0.20380523800849915            R_loss 6.233788967132568, KLD_loss 0.28797879815101624\n",
            "Train Epoch: 10 [9600/14166 (67.72009029345372%)]             Loss:0.2016388177871704            R_loss 6.164669036865234, KLD_loss 0.28777313232421875\n",
            "Train Epoch: 10 [9920/14166 (69.97742663656885%)]             Loss:0.1982293725013733            R_loss 6.129039764404297, KLD_loss 0.21430030465126038\n",
            "Train Epoch: 10 [10240/14166 (72.23476297968398%)]             Loss:0.20157518982887268            R_loss 6.208336353302002, KLD_loss 0.24206958711147308\n",
            "Train Epoch: 10 [10560/14166 (74.49209932279909%)]             Loss:0.20312434434890747            R_loss 6.331040382385254, KLD_loss 0.16893871128559113\n",
            "Train Epoch: 10 [10880/14166 (76.74943566591422%)]             Loss:0.2128734588623047            R_loss 6.524789810180664, KLD_loss 0.28716060519218445\n",
            "Train Epoch: 10 [11200/14166 (79.00677200902935%)]             Loss:0.2189139425754547            R_loss 6.648581504821777, KLD_loss 0.3566650450229645\n",
            "Train Epoch: 10 [11520/14166 (81.26410835214448%)]             Loss:0.20460321009159088            R_loss 6.269376277923584, KLD_loss 0.27792611718177795\n",
            "Train Epoch: 10 [11840/14166 (83.52144469525959%)]             Loss:0.2021060436964035            R_loss 6.236937522888184, KLD_loss 0.23045574128627777\n",
            "Train Epoch: 10 [12160/14166 (85.77878103837472%)]             Loss:0.20678377151489258            R_loss 6.401275157928467, KLD_loss 0.21580512821674347\n",
            "Train Epoch: 10 [12480/14166 (88.03611738148985%)]             Loss:0.2149515450000763            R_loss 6.578105449676514, KLD_loss 0.30034470558166504\n",
            "Train Epoch: 10 [12800/14166 (90.29345372460497%)]             Loss:0.21119700372219086            R_loss 6.416769027709961, KLD_loss 0.341534823179245\n",
            "Train Epoch: 10 [13120/14166 (92.55079006772009%)]             Loss:0.2037954479455948            R_loss 6.253186225891113, KLD_loss 0.2682676911354065\n",
            "Train Epoch: 10 [13440/14166 (94.80812641083521%)]             Loss:0.20743133127689362            R_loss 6.36977481842041, KLD_loss 0.2680278718471527\n",
            "Train Epoch: 10 [13760/14166 (97.06546275395034%)]             Loss:0.20372575521469116            R_loss 6.278732776641846, KLD_loss 0.24049055576324463\n",
            "Train Epoch: 10 [14080/14166 (99.32279909706546%)]             Loss:0.21187113225460052            R_loss 6.399740219116211, KLD_loss 0.38013577461242676\n",
            "====> Epoch: 10 Average loss: 0.2078\n",
            "====> Test set loss: 0.2079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L3NKOc3mHlIP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}